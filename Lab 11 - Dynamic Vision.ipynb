{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7bc57d1-e9d8-4129-8e5b-f0ea61c49327",
   "metadata": {},
   "source": [
    "# 2110433 - Computer Vision (2023/2)\n",
    "## Lab 11 - Dynamic Vision & 3D\n",
    "In this lab, we will learn to describe image motion from images. This notebook includes both coding and written questions. Please hand in this notebook file with all outputs and your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f51e97-46d8-4886-a443-bf654c310ecf",
   "metadata": {},
   "source": [
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a6184-9bca-4eed-80a6-722b27f1a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be31d0-b286-4a28-8307-fbecec33526c",
   "metadata": {},
   "source": [
    "## 1. Background Subtraction\n",
    "This tutorial was originally from [here](https://docs.opencv.org/3.4/d1/dc5/tutorial_background_subtraction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a1153-ae12-42d9-9f28-511bc5bbc4fe",
   "metadata": {},
   "source": [
    "- Background subtraction (BS) is a common and widely used technique for generating a foreground mask (namely, a binary image containing the pixels belonging to moving objects in the scene) by using static cameras.\n",
    "- As the name suggests, BS calculates the foreground mask performing a subtraction between the current frame and a background model, containing the static part of the scene or, more in general, everything that can be considered as background given the characteristics of the observed scene.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259566ce-4415-43dc-8efa-b00723a7f2e7",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.opencv.org/3.4/Background_Subtraction_Tutorial_Scheme.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26baa5b-008c-4786-93b0-7607b67659e1",
   "metadata": {},
   "source": [
    "A [cv2.VideoCapture](https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html) object is used to read the input video or input images sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536818dc-ebc1-44e7-affd-6cf4bfbc36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('assets/vtest.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f84c27-b216-4545-b444-c726b9435443",
   "metadata": {},
   "source": [
    "A [cv2.BackgroundSubtractor](https://docs.opencv.org/3.4/d7/df6/classcv_1_1BackgroundSubtractor.html) object will be used to generate the foreground mask. In this example, default parameters are used, but it is also possible to declare specific parameters in the create function.\n",
    "\n",
    "We use MOG background subtractor for this example.\n",
    "\n",
    "It uses a method to model each background pixel by a mixture of K Gaussian distributions (K = 3 to 5). The weights of the mixture represent the time proportions that those colours stay in the scene. The probable background colours are the ones which stay longer and more static.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*1EkPhFtPTRgmHumI.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b191306-0341-4489-b2cd-218eef02daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "backSub = cv2.bgsegm.createBackgroundSubtractorMOG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8639bab-b16d-4442-b7ae-601bb4f029e5",
   "metadata": {},
   "source": [
    "Every frame is used both for calculating the foreground mask and for updating the background. If you want to change the learning rate used for updating the background model, it is possible to set a specific learning rate by passing a parameter to the apply method.\n",
    "\n",
    "You can read more about \"apply\" function by clicking [here](https://docs.opencv.org/3.4/d7/df6/classcv_1_1BackgroundSubtractor.html#aa735e76f7069b3fa9c3f32395f9ccd21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5d56d-b6ab-4fc7-862f-23127f621402",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, frame = capture.read()\n",
    "fgMask = backSub.apply(frame)\n",
    "plt.imshow(fgMask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a52cf0-4bce-43b2-9662-5ab6678cd434",
   "metadata": {},
   "source": [
    "Show the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6c322-412f-44d9-996d-e99d34070e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('assets/vtest.avi')\n",
    "backSub = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    fgMask = backSub.apply(frame)\n",
    "    \n",
    "    \n",
    "    cv2.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n",
    "    cv2.putText(frame, str(capture.get(cv2.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "    \n",
    "    \n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('KNN Mask', fgMask)\n",
    "    \n",
    "    keyboard = cv2.waitKey(30)\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91042a90-c3bf-4d6b-b485-dd0767accdd2",
   "metadata": {},
   "source": [
    "There are other background subtraction algorithms as shown in the image below.\n",
    "\n",
    "<img src=\"https://docs.opencv.org/3.4/d7/df6/classcv_1_1BackgroundSubtractor.png\"></img>\n",
    "\n",
    "In the next block, we compare 4 background subtraction algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e73c8-967c-45ca-b941-0b9d5a1d2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('assets/vtest.avi')\n",
    "\n",
    "\n",
    "backSubKNN = cv2.createBackgroundSubtractorKNN()\n",
    "backSubMOG2 = cv2.createBackgroundSubtractorMOG2()\n",
    "backSubMOG = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "backSubGMG = cv2.bgsegm.createBackgroundSubtractorGMG()\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame, (400, 300)) \n",
    "    \n",
    "    fgMaskKNN = backSubKNN.apply(frame)\n",
    "    fgMaskMOG = backSubMOG.apply(frame)\n",
    "    fgMaskMOG2 = backSubMOG2.apply(frame)\n",
    "    fgMaskGMG = backSubGMG.apply(frame)\n",
    "    \n",
    "    \n",
    "    cv2.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n",
    "    cv2.putText(frame, str(capture.get(cv2.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "    \n",
    "    \n",
    "    original =  cv2.imshow('Frame', frame)\n",
    "    \n",
    "    \n",
    "    cv2.imshow('KNN Mask', fgMaskKNN)\n",
    "    cv2.imshow('MOG Mask', fgMaskMOG)\n",
    "    cv2.imshow('MOG2 Mask', fgMaskMOG2)\n",
    "    cv2.imshow('GMG Mask', fgMaskGMG)\n",
    "    \n",
    "    keyboard = cv2.waitKey(30)\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd23a5b-feb2-4c75-8819-fb79c6bf5320",
   "metadata": {},
   "source": [
    "## 2. Optical Flow in OpenCV\n",
    "This tutorial was originally from [here](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de78ea4-9d53-4648-84f1-e5c231bc745a",
   "metadata": {},
   "source": [
    "Optical flow is the pattern of apparent motion of image objects between two consecutive frames caused by the movement of object or camera. It is 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second. Consider the image below (Image Courtesy: [Wikipedia article on Optical Flow](https://en.wikipedia.org/wiki/Optical_flow)).\n",
    "\n",
    "<img src='https://docs.opencv.org/3.4/optical_flow_basic1.jpg'/>\n",
    "image\n",
    "It shows a ball moving in 5 consecutive frames. The arrow shows its displacement vector. \n",
    "\n",
    "\n",
    "\n",
    "Optical flow works on several assumptions:\n",
    "- The pixel intensities of an object do not change between consecutive frames.\n",
    "- Neighbouring pixels have similar motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490da4eb-d166-4caa-833a-c23a92de511d",
   "metadata": {},
   "source": [
    "### Lucas-Kanade Optical Flow \n",
    "\n",
    "We have seen an assumption before, that all the neighbouring pixels will have similar motion. Lucas-Kanade method takes a 3x3 patch around the point. So all the 9 points have the same motion.\n",
    "\n",
    "So from the user point of view, the idea is simple, we give some points to track, we receive the optical flow vectors of those points. But again there are some problems. Until now, we were dealing with small motions, so it fails when there is a large motion. To deal with this we use pyramids. When we go up in the pyramid, small motions are removed and large motions become small motions. So by applying Lucas-Kanade there, we get optical flow along with the scale.\n",
    "\n",
    "\n",
    "You can see the video below for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093e4d5-4af6-4ddf-88f0-fc7d372d2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(id=\"yFX_N5p0kO0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a48fc-3848-40f2-8fd1-450afc16002a",
   "metadata": {},
   "source": [
    "To decide the points to detect optical flow, we use [cv2.goodFeaturesToTrack()](https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541). We take the first frame, detect some Shi-Tomasi corner points in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37853e96-4251-48ce-9375-a45c247662d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('assets/slow_traffic_small.mp4')\n",
    "\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8489ff-155a-4e17-9604-60c7400a04a2",
   "metadata": {},
   "source": [
    "Then, we iteratively track those points using Lucas-Kanade optical flow. For the function [cv2.calcOpticalFlowPyrLK()](https://docs.opencv.org/3.4/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323) we pass the previous frame, previous points and next frame. It returns next points along with some status numbers which has a value of 1 if next point is found, else zero. We iteratively pass these next points as previous points in next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2e5ec-daf2-46a2-bfb7-7291ae11e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15, 15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "# Create some random colors\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "    # Select good points\n",
    "    if p1 is not None:\n",
    "        good_new = p1[st==1]\n",
    "        good_old = p0[st==1]\n",
    "    # draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "    img = cv2.add(frame, mask)\n",
    "    cv2.imshow('frame', img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0892dac-375f-412b-808b-3814ce65849f",
   "metadata": {},
   "source": [
    "#### Dense Optical Flow\n",
    "\n",
    "Lucas-Kanade method computes optical flow for a sparse feature set (in our example, corners detected using Shi-Tomasi algorithm). OpenCV provides another algorithm to find the dense optical flow. It computes the optical flow for all the points in the frame. It is based on Gunnar Farneback's algorithm which is explained in \"Two-Frame Motion Estimation Based on Polynomial Expansion\" by Gunnar Farneback in 2003.\n",
    "\n",
    "Below sample shows how to find the dense optical flow using above algorithm. We get a 2-channel array with optical flow vectors, (u,v). We find their magnitude and direction. We color code the result for better visualization. Direction corresponds to Hue value of the image. Magnitude corresponds to Value plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bba680-809b-4e90-8a9e-2de58de26a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(\"assets/slow_traffic_small.mp4\")\n",
    "ret, frame1 = cap.read()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    cv2.imshow('Frame', frame2)\n",
    "    cv2.imshow('frame2', bgr)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "   \n",
    "    prvs = next\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5c6aa",
   "metadata": {},
   "source": [
    "## 3. Camera Calibration\n",
    "This tutorial was originally from [here](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33dfac2",
   "metadata": {},
   "source": [
    "<div class=\"textblock\"><h1><a class=\"anchor\" id=\"autotoc_md1157\"></a>\n",
    "Goal</h1>\n",
    "<p>In this section, we will learn about</p>\n",
    "<ul>\n",
    "<li>types of distortion caused by cameras</li>\n",
    "<li>how to find the intrinsic and extrinsic properties of a camera</li>\n",
    "<li>how to undistort images based off these properties</li>\n",
    "</ul>\n",
    "<h1><a class=\"anchor\" id=\"autotoc_md1158\"></a>\n",
    "Basics</h1>\n",
    "<p>Some pinhole cameras introduce significant distortion to images. Two major kinds of distortion are radial distortion and tangential distortion.</p>\n",
    "<p>Radial distortion causes straight lines to appear curved. Radial distortion becomes larger the farther points are from the center of the image. For example, one image is shown below in which two edges of a chess board are marked with red lines. But, you can see that the border of the chess board is not a straight line and doesn't match with the red line. All the expected straight lines are bulged out. Visit <a href=\"https://en.wikipedia.org/wiki/Distortion_%28optics%29\" target=\"_blank\" class=\"keychainify-checked\">Distortion (optics)</a> for more details.</p>\n",
    "<div class=\"image\">\n",
    "<img src=\"https://docs.opencv.org/4.x/calib_radial.jpg\" alt=\"\">\n",
    "<div class=\"caption\">\n",
    "image</div></div>\n",
    "<p>Radial distortion can be represented as follows:</p>\n",
    "<p class=\"formulaDsp\">\n",
    "<mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"0\" style=\"font-size: 118.2%; position: relative;\"><mjx-math display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-texatom size=\"s\" texclass=\"ORD\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c64\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c69\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c73\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6F\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c65\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c64\"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"4\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-msub space=\"3\"><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c6B\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-msub space=\"3\"><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c6B\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c34\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-msub space=\"3\"><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c6B\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c33\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c36\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mspace></mjx-mspace><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-texatom size=\"s\" texclass=\"ORD\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c64\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c69\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c73\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6F\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c65\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c64\"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"4\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-msub space=\"3\"><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c6B\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-msub space=\"3\"><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c6B\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c34\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-msub space=\"3\"><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c6B\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c33\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c36\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>x</mi><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>x</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><msub><mi>k</mi><mn>1</mn></msub><msup><mi>r</mi><mn>2</mn></msup><mo>+</mo><msub><mi>k</mi><mn>2</mn></msub><msup><mi>r</mi><mn>4</mn></msup><mo>+</mo><msub><mi>k</mi><mn>3</mn></msub><msup><mi>r</mi><mn>6</mn></msup><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><msub><mi>y</mi><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>y</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><msub><mi>k</mi><mn>1</mn></msub><msup><mi>r</mi><mn>2</mn></msup><mo>+</mo><msub><mi>k</mi><mn>2</mn></msub><msup><mi>r</mi><mn>4</mn></msup><mo>+</mo><msub><mi>k</mi><mn>3</mn></msub><msup><mi>r</mi><mn>6</mn></msup><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container>\n",
    "</p>\n",
    "<p>Similarly, tangential distortion occurs because the image-taking lense is not aligned perfectly parallel to the imaging plane. So, some areas in the image may look nearer than expected. The amount of tangential distortion can be represented as below:</p>\n",
    "<p class=\"formulaDsp\">\n",
    "<mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"1\" style=\"font-size: 118.2%; position: relative;\"><mjx-math display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-texatom size=\"s\" texclass=\"ORD\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c64\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c69\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c73\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6F\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c65\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c64\"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"4\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c5B\"></mjx-c></mjx-mo><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c70\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-msub space=\"3\"><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c70\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-mn class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c5D\"></mjx-c></mjx-mo><mjx-mspace></mjx-mspace><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-texatom size=\"s\" texclass=\"ORD\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c64\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c69\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c73\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6F\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c65\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c64\"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"4\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c5B\"></mjx-c></mjx-mo><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c70\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-mn class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.413em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c2B\"></mjx-c></mjx-mo><mjx-mn class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c70\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c5D\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>x</mi><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>x</mi><mo>+</mo><mo stretchy=\"false\">[</mo><mn>2</mn><msub><mi>p</mi><mn>1</mn></msub><mi>x</mi><mi>y</mi><mo>+</mo><msub><mi>p</mi><mn>2</mn></msub><mo stretchy=\"false\">(</mo><msup><mi>r</mi><mn>2</mn></msup><mo>+</mo><mn>2</mn><msup><mi>x</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mspace linebreak=\"newline\"></mspace><msub><mi>y</mi><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>y</mi><mo>+</mo><mo stretchy=\"false\">[</mo><msub><mi>p</mi><mn>1</mn></msub><mo stretchy=\"false\">(</mo><msup><mi>r</mi><mn>2</mn></msup><mo>+</mo><mn>2</mn><msup><mi>y</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo><mo>+</mo><mn>2</mn><msub><mi>p</mi><mn>2</mn></msub><mi>x</mi><mi>y</mi><mo stretchy=\"false\">]</mo></math></mjx-assistive-mml></mjx-container>\n",
    "</p>\n",
    "<p>In short, we need to find five parameters, known as distortion coefficients given by:</p>\n",
    "<p class=\"formulaDsp\">\n",
    "<mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"2\" style=\"font-size: 118.2%; position: relative;\"><mjx-math display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c44\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c69\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c73\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6F\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c69\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6F\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6E\"></mjx-c></mjx-mi><mjx-mstyle><mjx-mspace style=\"width: 0.278em;\"></mjx-mspace></mjx-mstyle><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c63\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6F\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c65\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c66\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c66\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c69\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c63\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c69\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c65\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6E\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c73\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c6B\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mspace style=\"width: 1em;\"></mjx-mspace><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c6B\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mspace style=\"width: 1em;\"></mjx-mspace><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c70\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mspace style=\"width: 1em;\"></mjx-mspace><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c70\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c32\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mspace style=\"width: 1em;\"></mjx-mspace><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c6B\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mn class=\"mjx-n\" size=\"s\"><mjx-c class=\"mjx-c33\"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>D</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mstyle scriptlevel=\"0\"><mspace width=\"thickmathspace\"></mspace></mstyle><mi>c</mi><mi>o</mi><mi>e</mi><mi>f</mi><mi>f</mi><mi>i</mi><mi>c</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>k</mi><mn>1</mn></msub><mspace width=\"10pt\"></mspace><msub><mi>k</mi><mn>2</mn></msub><mspace width=\"10pt\"></mspace><msub><mi>p</mi><mn>1</mn></msub><mspace width=\"10pt\"></mspace><msub><mi>p</mi><mn>2</mn></msub><mspace width=\"10pt\"></mspace><msub><mi>k</mi><mn>3</mn></msub><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container>\n",
    "</p>\n",
    "<p>In addition to this, we need to some other information, like the intrinsic and extrinsic parameters of the camera. Intrinsic parameters are specific to a camera. They include information like focal length ( <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"3\" style=\"font-size: 118.2%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c66\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2C\"></mjx-c></mjx-mo><mjx-msub space=\"2\"><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c66\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mi>x</mi></msub><mo>,</mo><msub><mi>f</mi><mi>y</mi></msub></math></mjx-assistive-mml></mjx-container>) and optical centers ( <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"4\" style=\"font-size: 118.2%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c63\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2C\"></mjx-c></mjx-mo><mjx-msub space=\"2\"><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c63\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>x</mi></msub><mo>,</mo><msub><mi>c</mi><mi>y</mi></msub></math></mjx-assistive-mml></mjx-container>). The focal length and optical centers can be used to create a camera matrix, which can be used to remove distortion due to the lenses of a specific camera. The camera matrix is unique to a specific camera, so once calculated, it can be reused on other images taken by the same camera. It is expressed as a 3x3 matrix:</p>\n",
    "<p class=\"formulaDsp\">\n",
    "<mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"5\" style=\"font-size: 118.2%; position: relative;\"><mjx-math display=\"true\" class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c63\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c61\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6D\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c65\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c61\"></mjx-c></mjx-mi><mjx-mstyle><mjx-mspace style=\"width: 0.278em;\"></mjx-mspace></mjx-mstyle><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c6D\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c61\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c72\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c69\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mrow space=\"4\"><mjx-mo class=\"mjx-n\"><mjx-stretchy-v class=\"mjx-c5B\" style=\"height: 3.845em; vertical-align: -1.672em;\"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo><mjx-mtable style=\"min-width: 4.718em;\"><mjx-table><mjx-itable><mjx-mtr><mjx-mtd style=\"padding-right: 0.5em; padding-bottom: 0.2em;\"><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c66\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-tstrut></mjx-tstrut></mjx-mtd><mjx-mtd style=\"padding-left: 0.5em; padding-right: 0.5em; padding-bottom: 0.2em;\"><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c30\"></mjx-c></mjx-mn><mjx-tstrut></mjx-tstrut></mjx-mtd><mjx-mtd style=\"padding-left: 0.5em; padding-bottom: 0.2em;\"><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c63\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c78\"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mtr><mjx-mtr><mjx-mtd style=\"padding-right: 0.5em; padding-top: 0.2em; padding-bottom: 0.2em;\"><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c30\"></mjx-c></mjx-mn><mjx-tstrut></mjx-tstrut></mjx-mtd><mjx-mtd style=\"padding: 0.2em 0.5em;\"><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c66\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-tstrut></mjx-tstrut></mjx-mtd><mjx-mtd style=\"padding-left: 0.5em; padding-top: 0.2em; padding-bottom: 0.2em;\"><mjx-msub><mjx-mi class=\"mjx-i\" noic=\"true\"><mjx-c class=\"mjx-c63\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c79\"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mtr><mjx-mtr><mjx-mtd style=\"padding-right: 0.5em; padding-top: 0.2em;\"><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c30\"></mjx-c></mjx-mn><mjx-tstrut></mjx-tstrut></mjx-mtd><mjx-mtd style=\"padding-left: 0.5em; padding-right: 0.5em; padding-top: 0.2em;\"><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c30\"></mjx-c></mjx-mn><mjx-tstrut></mjx-tstrut></mjx-mtd><mjx-mtd style=\"padding-left: 0.5em; padding-top: 0.2em;\"><mjx-mn class=\"mjx-n\"><mjx-c class=\"mjx-c31\"></mjx-c></mjx-mn><mjx-tstrut></mjx-tstrut></mjx-mtd></mjx-mtr></mjx-itable></mjx-table></mjx-mtable><mjx-mo class=\"mjx-n\"><mjx-stretchy-v class=\"mjx-c5D\" style=\"height: 3.845em; vertical-align: -1.672em;\"><mjx-beg><mjx-c></mjx-c></mjx-beg><mjx-ext><mjx-c></mjx-c></mjx-ext><mjx-end><mjx-c></mjx-c></mjx-end><mjx-mark></mjx-mark></mjx-stretchy-v></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>c</mi><mi>a</mi><mi>m</mi><mi>e</mi><mi>r</mi><mi>a</mi><mstyle scriptlevel=\"0\"><mspace width=\"thickmathspace\"></mspace></mstyle><mi>m</mi><mi>a</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>x</mi><mo>=</mo><mrow data-mjx-texclass=\"INNER\"><mo data-mjx-texclass=\"OPEN\">[</mo><mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mtr><mtd><msub><mi>f</mi><mi>x</mi></msub></mtd><mtd><mn>0</mn></mtd><mtd><msub><mi>c</mi><mi>x</mi></msub></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><msub><mi>f</mi><mi>y</mi></msub></mtd><mtd><msub><mi>c</mi><mi>y</mi></msub></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable><mo data-mjx-texclass=\"CLOSE\">]</mo></mrow></math></mjx-assistive-mml></mjx-container>\n",
    "</p>\n",
    "<p>Extrinsic parameters corresponds to rotation and translation vectors which translates a coordinates of a 3D point to a coordinate system.</p>\n",
    "<p>For stereo applications, these distortions need to be corrected first. To find these parameters, we must provide some sample images of a well defined pattern (e.g. a chess board). We find some specific points of which we already know the relative positions (e.g. square corners in the chess board). We know the coordinates of these points in real world space and we know the coordinates in the image, so we can solve for the distortion coefficients. For better results, we need at least 10 test patterns.</p>\n",
    "<h1><a class=\"anchor\" id=\"autotoc_md1159\"></a>\n",
    "Code</h1>\n",
    "<p>As mentioned above, we need at least 10 test patterns for camera calibration. OpenCV comes with some images of a chess board (see samples/data/left01.jpg – left14.jpg), so we will utilize these. Consider an image of a chess board. The important input data needed for calibration of the camera is the set of 3D real world points and the corresponding 2D coordinates of these points in the image. 2D image points are OK which we can easily find from the image. (These image points are locations where two black squares touch each other in chess boards)</p>\n",
    "<p>What about the 3D points from real world space? Those images are taken from a static camera and chess boards are placed at different locations and orientations. So we need to know <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"6\" style=\"font-size: 118.2%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c58\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2C\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"2\"><mjx-c class=\"mjx-c59\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2C\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"2\"><mjx-c class=\"mjx-c5A\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container> values. But for simplicity, we can say chess board was kept stationary at XY plane, (so Z=0 always) and camera was moved accordingly. This consideration helps us to find only X,Y values. Now for X,Y values, we can simply pass the points as (0,0), (1,0), (2,0), ... which denotes the location of points. In this case, the results we get will be in the scale of size of chess board square. But if we know the square size, (say 30 mm), we can pass the values as (0,0), (30,0), (60,0), ... . Thus, we get the results in mm. (In this case, we don't know square size since we didn't take those images, so we pass in terms of square size).</p>\n",
    "<p>3D points are called <b>object points</b> and 2D image points are called <b>image points.</b></p>\n",
    "<h2><a class=\"anchor\" id=\"autotoc_md1160\"></a>\n",
    "Setup</h2>\n",
    "<p>So to find pattern in chess board, we can use the function, <b><a class=\"el keychainify-checked\" href=\"https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a\" title=\"Finds the positions of internal corners of the chessboard.\">cv.findChessboardCorners()</a></b>. We also need to pass what kind of pattern we are looking for, like 8x8 grid, 5x5 grid etc. In this example, we use 7x6 grid. (Normally a chess board has 8x8 squares and 7x7 internal corners). It returns the corner points and retval which will be True if pattern is obtained. These corners will be placed in an order (from left-to-right, top-to-bottom)</p>\n",
    "<dl class=\"section note\"><dt>Note</dt><dd>This function may not be able to find the required pattern in all the images. So, one good option is to write the code such that, it starts the camera and check each frame for required pattern. Once the pattern is obtained, find the corners and store it in a list. Also, provide some interval before reading next frame so that we can adjust our chess board in different direction. Continue this process until the required number of good patterns are obtained. Even in the example provided here, we are not sure how many images out of the 14 given are good. Thus, we must read all the images and take only the good ones.</dd>\n",
    "<dd>\n",
    "Instead of chess board, we can alternatively use a circular grid. In this case, we must use the function <b><a class=\"el keychainify-checked\" href=\"https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga7f02cd21c8352142890190227628fa80\" title=\"Finds centers in the grid of circles.\">cv.findCirclesGrid()</a></b> to find the pattern. Fewer images are sufficient to perform camera calibration using a circular grid.</dd></dl>\n",
    "<p>Once we find the corners, we can increase their accuracy using <b><a class=\"el keychainify-checked\" href=\"https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga354e0d7c86d0d9da75de9b9701a9a87e\" title=\"Refines the corner locations.\">cv.cornerSubPix()</a></b>. We can also draw the pattern using <b><a class=\"el keychainify-checked\" href=\"https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga6a10b0bb120c4907e5eabbcd22319022\" title=\"Renders the detected chessboard corners.\">cv.drawChessboardCorners()</a></b>. All these steps are included in below code:</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff68986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    " \n",
    "# termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    " \n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*7,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\n",
    " \n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    " \n",
    "images = glob.glob('assets/left*.jpg')\n",
    "print(images)\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (7,6), None)\n",
    "\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "\n",
    "        corners2 = cv2.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria)\n",
    "        imgpoints.append(corners2)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (7,6), corners2, ret)\n",
    "        cv2.imshow('img', img)\n",
    "        cv2.waitKey(500)\n",
    " \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f21518",
   "metadata": {},
   "source": [
    "<h2><a class=\"anchor\" id=\"autotoc_md1161\"></a>\n",
    "Calibration</h2>\n",
    "<p>Now that we have our object points and image points, we are ready to go for calibration. We can use the function, <b><a class=\"el keychainify-checked\" href=\"https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d\" title=\"Finds the camera intrinsic and extrinsic parameters from several views of a calibration pattern.\">cv.calibrateCamera()</a></b> which returns the camera matrix, distortion coefficients, rotation and translation vectors etc. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa856538",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abcfab",
   "metadata": {},
   "source": [
    "<h2><a class=\"anchor\" id=\"autotoc_md1162\"></a>\n",
    "Undistortion</h2>\n",
    "<p>Now, we can take an image and undistort it. OpenCV comes with two methods for doing this. However first, we can refine the camera matrix based on a free scaling parameter using <b><a class=\"el keychainify-checked\" href=\"https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga7a6c4e032c97f03ba747966e6ad862b1\" title=\"Returns the new camera intrinsic matrix based on the free scaling parameter.\">cv.getOptimalNewCameraMatrix()</a></b>. If the scaling parameter alpha=0, it returns undistorted image with minimum unwanted pixels. So it may even remove some pixels at image corners. If alpha=1, all pixels are retained with some extra black images. This function also returns an image ROI which can be used to crop the result.</p>\n",
    "<p>So, we take a new image (left12.jpg in this case. That is the first image in this chapter) </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89656490",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('assets/left12.jpg')\n",
    "h, w = img.shape[:2]\n",
    "newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0a326",
   "metadata": {},
   "source": [
    "<h3><a class=\"anchor\" id=\"autotoc_md1163\"></a>\n",
    "Using cv.undistort()</h3>\n",
    "<p>This is the easiest way. Just call the function and use ROI obtained above to crop the result. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d90fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undistort\n",
    "dst = cv2.undistort(img, mtx, dist, None, newcameramtx)\n",
    " \n",
    "# crop the image\n",
    "x, y, w, h = roi\n",
    "dst = dst[y:y+h, x:x+w]\n",
    "cv2.imwrite('calibresult.png', dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45784c50",
   "metadata": {},
   "source": [
    "<p> Still, both the methods give the same result. See the result below:</p>\n",
    "<div class=\"image\">\n",
    "<img src=\"https://docs.opencv.org/4.x/calib_result.jpg\" alt=\"\">\n",
    "<div class=\"caption\">\n",
    "image</div></div>\n",
    "<p>You can see in the result that all the edges are straight.</p>\n",
    "<p>Now you can store the camera matrix and distortion coefficients using write functions in NumPy (np.savez, np.savetxt etc) for future uses.</p>\n",
    "<h1><a class=\"anchor\" id=\"autotoc_md1165\"></a>\n",
    "Re-projection Error</h1>\n",
    "<p>Re-projection error gives a good estimation of just how exact the found parameters are. The closer the re-projection error is to zero, the more accurate the parameters we found are. Given the intrinsic, distortion, rotation and translation matrices, we must first transform the object point to image point using <b><a class=\"el keychainify-checked\" href=\"https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c\" title=\"Projects 3D points to an image plane.\">cv.projectPoints()</a></b>. Then, we can calculate the absolute norm between what we got with our transformation and the corner finding algorithm. To find the average error, we calculate the arithmetical mean of the errors calculated for all the calibration images. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908784a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_error = 0\n",
    "for i in range(len(objpoints)):\n",
    " imgpoints2, _ = cv2.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist)\n",
    " error = cv2.norm(imgpoints[i], imgpoints2, cv2.NORM_L2)/len(imgpoints2)\n",
    " mean_error += error\n",
    " \n",
    "print( \"total error: {}\".format(mean_error/len(objpoints)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727176f",
   "metadata": {},
   "source": [
    "## 4. Depth Map from Stereo Images\n",
    "This tutorial was originally from [here](https://docs.opencv.org/4.x/dd/d53/tutorial_py_depthmap.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb9945",
   "metadata": {},
   "source": [
    "Below code snippet shows a simple procedure to create a disparity map.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dce840",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgL = cv2.imread('assets/tsukuba_l.png', cv2.IMREAD_GRAYSCALE)\n",
    "imgR = cv2.imread('assets/tsukuba_r.png', cv2.IMREAD_GRAYSCALE)\n",
    " \n",
    "stereo = cv2.StereoBM.create(numDisparities=16, blockSize=15)\n",
    "disparity = stereo.compute(imgL,imgR)\n",
    "plt.imshow(disparity,'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265f577",
   "metadata": {},
   "source": [
    "There are some parameters when you get familiar with StereoBM, and you may need to fine tune the parameters to get better and smooth results. Parameters:\n",
    "\n",
    "- texture_threshold: filters out areas that don't have enough texture for reliable matching\n",
    "- Speckle range and size: Block-based matchers often produce \"speckles\" near the boundaries of objects, where the matching window catches the foreground on one side and the background on the other. In this scene it appears that the matcher is also finding small spurious matches in the projected texture on the table. To get rid of these artifacts we post-process the disparity image with a speckle filter controlled by the speckle_size and speckle_range parameters. speckle_size is the number of pixels below which a disparity blob is dismissed as \"speckle.\" speckle_range controls how close in value disparities must be to be considered part of the same blob.\n",
    "- Number of disparities: How many pixels to slide the window over. The larger it is, the larger the range of visible depths, but more computation is required.\n",
    "- min_disparity: the offset from the x-position of the left pixel at which to begin searching.\n",
    "- uniqueness_ratio: Another post-filtering step. If the best matching disparity is not sufficiently better than every other disparity in the search range, the pixel is filtered out. You can try tweaking this if texture_threshold and the speckle filtering are still letting through spurious matches.\n",
    "- prefilter_size and prefilter_cap: The pre-filtering phase, which normalizes image brightness and enhances texture in preparation for block matching. Normally you should not need to adjust these.\n",
    "These parameters are set with dedicated setters and getters after the algoritm initialization, such as setTextureThreshold, setSpeckleRange, setUniquenessRatio, and more. See cv::StereoBM documentation for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd50494",
   "metadata": {},
   "source": [
    "These parameters are set with dedicated setters and getters after the algoritm initialization, such as setTextureThreshold, setSpeckleRange, setUniquenessRatio, and more. See cv::StereoBM documentation for details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgL = cv2.imread('assets/plantL.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "imgR = cv2.imread('assets/plantR.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(imgR,'gray')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "stereo = cv2.StereoBM.create(numDisparities=128, blockSize=15)\n",
    "disparity = stereo.compute(imgL,imgR)\n",
    "plt.imshow(disparity,'gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
